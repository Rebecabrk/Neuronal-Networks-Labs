{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import bernoulli\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform=lambda x: np.array(x).flatten() / 255.0,\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return np.array(mnist_data), np.array(mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    return encoder.fit_transform(labels.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initialization_Xavier_Uniform(input_dim, output_dim):\n",
    "    low_bound = -math.sqrt(6/(input_dim + output_dim))\n",
    "    upper_bound = math.sqrt(6/(input_dim + output_dim))\n",
    "    W = np.random.uniform(low_bound, upper_bound, size=(input_dim, output_dim))\n",
    "    b = np.zeros((1, output_dim))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, W1, b1, W2, b2, dropout_rate=0.35):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    # Implement dropout_rate% dropout\n",
    "    dropouts = bernoulli.rvs(dropout_rate, size=A1.shape) \n",
    "\n",
    "    A1_dropout = A1 * dropouts / (1 - dropout_rate)\n",
    "\n",
    "    Z2 = np.dot(A1_dropout, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1_dropout, A2, dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_no_dropout(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_cross_entropy(A, Y):\n",
    "    m = Y.shape[0]\n",
    "    log_likelihood = -np.log(A[range(m), Y.argmax(axis=1)])\n",
    "    return np.sum(log_likelihood) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, A2, A1, W2, dropouts):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Output layer error term (dZ2)\n",
    "    dZ2 = A2 - Y  # Derivative of softmax + cross-entropy\n",
    "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient w.r.t. weights (output layer)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient w.r.t. biases (output layer)\n",
    "    \n",
    "    # Hidden layer error term (dZ1)\n",
    "    dZ1 = np.dot(dZ2, W2.T) * A1 * (1 - A1)  # For sigmoid activation (derivative of sigmoid)\n",
    "    \n",
    "    dZ1 = dZ1 * dropouts\n",
    "    dZ1 /= (1 - 0.35)\n",
    "\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # Gradient w.r.t. weights (hidden layer)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient w.r.t. biases (hidden layer)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    \n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X, train_Y, input_dim, intermediary_dim, output_dim, epochs=100, learning_rate=0.01, batch_size=100):\n",
    "    W1, b1 = weight_initialization_Xavier_Uniform(input_dim, intermediary_dim)\n",
    "    W2, b2 = weight_initialization_Xavier_Uniform(intermediary_dim, output_dim)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(train_X.shape[0])\n",
    "        train_X = train_X[perm]\n",
    "        train_Y = train_Y[perm]\n",
    "        \n",
    "        for i in range(0, train_X.shape[0], batch_size):\n",
    "            X_batch = train_X[i:i+batch_size]\n",
    "            Y_batch = train_Y[i:i+batch_size]\n",
    "            \n",
    "            # Forward propagation\n",
    "            A1, A2, dropouts = forward_propagation_with_dropout(X_batch, W1, b1, W2, b2) \n",
    "            \n",
    "            # Compute loss (optional for tracking)\n",
    "            loss = compute_loss_cross_entropy(A2, Y_batch)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = backward_propagation(X_batch, Y_batch, A2, A1, W2, dropouts) \n",
    "            \n",
    "            # Update parameters\n",
    "            W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W1, b1, W2, b2):\n",
    "    A2 = forward_propagation_no_dropout(X, W1, b1, W2, b2)[1]\n",
    "    return np.argmax(A2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 2.3044\n",
      "Epoch 2/200, Loss: 2.0566\n",
      "Epoch 3/200, Loss: 2.0213\n",
      "Epoch 4/200, Loss: 1.7431\n",
      "Epoch 5/200, Loss: 1.7413\n",
      "Epoch 6/200, Loss: 1.6215\n",
      "Epoch 7/200, Loss: 1.4943\n",
      "Epoch 8/200, Loss: 1.4184\n",
      "Epoch 9/200, Loss: 1.3101\n",
      "Epoch 10/200, Loss: 1.2578\n",
      "Epoch 11/200, Loss: 1.2551\n",
      "Epoch 12/200, Loss: 1.0637\n",
      "Epoch 13/200, Loss: 1.0971\n",
      "Epoch 14/200, Loss: 1.0770\n",
      "Epoch 15/200, Loss: 1.0309\n",
      "Epoch 16/200, Loss: 0.9130\n",
      "Epoch 17/200, Loss: 1.0021\n",
      "Epoch 18/200, Loss: 0.9399\n",
      "Epoch 19/200, Loss: 0.8998\n",
      "Epoch 20/200, Loss: 0.9349\n",
      "Epoch 21/200, Loss: 0.9425\n",
      "Epoch 22/200, Loss: 0.7922\n",
      "Epoch 23/200, Loss: 0.7923\n",
      "Epoch 24/200, Loss: 0.9429\n",
      "Epoch 25/200, Loss: 0.8534\n",
      "Epoch 26/200, Loss: 0.8514\n",
      "Epoch 27/200, Loss: 0.8476\n",
      "Epoch 28/200, Loss: 0.7547\n",
      "Epoch 29/200, Loss: 0.8239\n",
      "Epoch 30/200, Loss: 0.8696\n",
      "Epoch 31/200, Loss: 0.7410\n",
      "Epoch 32/200, Loss: 0.9314\n",
      "Epoch 33/200, Loss: 0.5896\n",
      "Epoch 34/200, Loss: 0.7242\n",
      "Epoch 35/200, Loss: 0.6228\n",
      "Epoch 36/200, Loss: 0.7004\n",
      "Epoch 37/200, Loss: 0.6669\n",
      "Epoch 38/200, Loss: 0.7248\n",
      "Epoch 39/200, Loss: 0.6691\n",
      "Epoch 40/200, Loss: 0.7561\n",
      "Epoch 41/200, Loss: 0.6385\n",
      "Epoch 42/200, Loss: 0.5538\n",
      "Epoch 43/200, Loss: 0.7307\n",
      "Epoch 44/200, Loss: 0.5879\n",
      "Epoch 45/200, Loss: 0.5611\n",
      "Epoch 46/200, Loss: 0.6139\n",
      "Epoch 47/200, Loss: 0.5203\n",
      "Epoch 48/200, Loss: 0.4238\n",
      "Epoch 49/200, Loss: 0.6454\n",
      "Epoch 50/200, Loss: 0.6521\n",
      "Epoch 51/200, Loss: 0.6495\n",
      "Epoch 52/200, Loss: 0.7069\n",
      "Epoch 53/200, Loss: 0.6237\n",
      "Epoch 54/200, Loss: 0.4999\n",
      "Epoch 55/200, Loss: 0.6257\n",
      "Epoch 56/200, Loss: 0.6599\n",
      "Epoch 57/200, Loss: 0.6160\n",
      "Epoch 58/200, Loss: 0.4492\n",
      "Epoch 59/200, Loss: 0.5485\n",
      "Epoch 60/200, Loss: 0.5681\n",
      "Epoch 61/200, Loss: 0.8255\n",
      "Epoch 62/200, Loss: 0.5694\n",
      "Epoch 63/200, Loss: 0.5327\n",
      "Epoch 64/200, Loss: 0.5254\n",
      "Epoch 65/200, Loss: 0.4947\n",
      "Epoch 66/200, Loss: 0.5999\n",
      "Epoch 67/200, Loss: 0.6248\n",
      "Epoch 68/200, Loss: 0.5257\n",
      "Epoch 69/200, Loss: 0.4118\n",
      "Epoch 70/200, Loss: 0.3772\n",
      "Epoch 71/200, Loss: 0.4153\n",
      "Epoch 72/200, Loss: 0.6475\n",
      "Epoch 73/200, Loss: 0.4280\n",
      "Epoch 74/200, Loss: 0.5057\n",
      "Epoch 75/200, Loss: 0.4535\n",
      "Epoch 76/200, Loss: 0.3923\n",
      "Epoch 77/200, Loss: 0.5602\n",
      "Epoch 78/200, Loss: 0.4872\n",
      "Epoch 79/200, Loss: 0.4035\n",
      "Epoch 80/200, Loss: 0.4743\n",
      "Epoch 81/200, Loss: 0.5365\n",
      "Epoch 82/200, Loss: 0.4614\n",
      "Epoch 83/200, Loss: 0.5652\n",
      "Epoch 84/200, Loss: 0.4940\n",
      "Epoch 85/200, Loss: 0.5783\n",
      "Epoch 86/200, Loss: 0.4649\n",
      "Epoch 87/200, Loss: 0.6793\n",
      "Epoch 88/200, Loss: 0.3648\n",
      "Epoch 89/200, Loss: 0.6295\n",
      "Epoch 90/200, Loss: 0.5625\n",
      "Epoch 91/200, Loss: 0.4640\n",
      "Epoch 92/200, Loss: 0.5213\n",
      "Epoch 93/200, Loss: 0.4585\n",
      "Epoch 94/200, Loss: 0.5091\n",
      "Epoch 95/200, Loss: 0.4757\n",
      "Epoch 96/200, Loss: 0.4861\n",
      "Epoch 97/200, Loss: 0.6203\n",
      "Epoch 98/200, Loss: 0.5259\n",
      "Epoch 99/200, Loss: 0.5839\n",
      "Epoch 100/200, Loss: 0.4154\n",
      "Epoch 101/200, Loss: 0.7293\n",
      "Epoch 102/200, Loss: 0.5461\n",
      "Epoch 103/200, Loss: 0.5172\n",
      "Epoch 104/200, Loss: 0.4077\n",
      "Epoch 105/200, Loss: 0.5782\n",
      "Epoch 106/200, Loss: 0.6466\n",
      "Epoch 107/200, Loss: 0.7532\n",
      "Epoch 108/200, Loss: 0.4259\n",
      "Epoch 109/200, Loss: 0.3341\n",
      "Epoch 110/200, Loss: 0.4599\n",
      "Epoch 111/200, Loss: 0.5203\n",
      "Epoch 112/200, Loss: 0.4792\n",
      "Epoch 113/200, Loss: 0.4988\n",
      "Epoch 114/200, Loss: 0.5006\n",
      "Epoch 115/200, Loss: 0.3297\n",
      "Epoch 116/200, Loss: 0.4390\n",
      "Epoch 117/200, Loss: 0.5442\n",
      "Epoch 118/200, Loss: 0.4420\n",
      "Epoch 119/200, Loss: 0.4581\n",
      "Epoch 120/200, Loss: 0.4354\n",
      "Epoch 121/200, Loss: 0.3104\n",
      "Epoch 122/200, Loss: 0.6946\n",
      "Epoch 123/200, Loss: 0.5639\n",
      "Epoch 124/200, Loss: 0.5193\n",
      "Epoch 125/200, Loss: 0.4510\n",
      "Epoch 126/200, Loss: 0.4367\n",
      "Epoch 127/200, Loss: 0.4686\n",
      "Epoch 128/200, Loss: 0.3100\n",
      "Epoch 129/200, Loss: 0.4797\n",
      "Epoch 130/200, Loss: 0.4360\n",
      "Epoch 131/200, Loss: 0.5776\n",
      "Epoch 132/200, Loss: 0.5175\n",
      "Epoch 133/200, Loss: 0.3142\n",
      "Epoch 134/200, Loss: 0.3590\n",
      "Epoch 135/200, Loss: 0.3590\n",
      "Epoch 136/200, Loss: 0.4990\n",
      "Epoch 137/200, Loss: 0.4180\n",
      "Epoch 138/200, Loss: 0.5422\n",
      "Epoch 139/200, Loss: 0.6853\n",
      "Epoch 140/200, Loss: 0.3748\n",
      "Epoch 141/200, Loss: 0.3820\n",
      "Epoch 142/200, Loss: 0.4500\n",
      "Epoch 143/200, Loss: 0.4332\n",
      "Epoch 144/200, Loss: 0.5688\n",
      "Epoch 145/200, Loss: 0.5545\n",
      "Epoch 146/200, Loss: 0.4267\n",
      "Epoch 147/200, Loss: 0.4712\n",
      "Epoch 148/200, Loss: 0.3114\n",
      "Epoch 149/200, Loss: 0.3979\n",
      "Epoch 150/200, Loss: 0.5255\n",
      "Epoch 151/200, Loss: 0.4190\n",
      "Epoch 152/200, Loss: 0.3820\n",
      "Epoch 153/200, Loss: 0.3687\n",
      "Epoch 154/200, Loss: 0.5287\n",
      "Epoch 155/200, Loss: 0.4063\n",
      "Epoch 156/200, Loss: 0.5856\n",
      "Epoch 157/200, Loss: 0.4577\n",
      "Epoch 158/200, Loss: 0.4755\n",
      "Epoch 159/200, Loss: 0.5567\n",
      "Epoch 160/200, Loss: 0.4328\n",
      "Epoch 161/200, Loss: 0.4231\n",
      "Epoch 162/200, Loss: 0.3376\n",
      "Epoch 163/200, Loss: 0.5261\n",
      "Epoch 164/200, Loss: 0.3457\n",
      "Epoch 165/200, Loss: 0.4988\n",
      "Epoch 166/200, Loss: 0.4656\n",
      "Epoch 167/200, Loss: 0.4251\n",
      "Epoch 168/200, Loss: 0.7898\n",
      "Epoch 169/200, Loss: 0.6693\n",
      "Epoch 170/200, Loss: 0.6205\n",
      "Epoch 171/200, Loss: 0.6338\n",
      "Epoch 172/200, Loss: 0.4960\n",
      "Epoch 173/200, Loss: 0.4256\n",
      "Epoch 174/200, Loss: 0.5093\n",
      "Epoch 175/200, Loss: 0.5344\n",
      "Epoch 176/200, Loss: 0.4957\n",
      "Epoch 177/200, Loss: 0.4601\n",
      "Epoch 178/200, Loss: 0.2733\n",
      "Epoch 179/200, Loss: 0.5845\n",
      "Epoch 180/200, Loss: 0.2841\n",
      "Epoch 181/200, Loss: 0.3241\n",
      "Epoch 182/200, Loss: 0.3998\n",
      "Epoch 183/200, Loss: 0.6033\n",
      "Epoch 184/200, Loss: 0.6400\n",
      "Epoch 185/200, Loss: 0.4469\n",
      "Epoch 186/200, Loss: 0.7342\n",
      "Epoch 187/200, Loss: 0.5099\n",
      "Epoch 188/200, Loss: 0.6960\n",
      "Epoch 189/200, Loss: 0.3159\n",
      "Epoch 190/200, Loss: 0.6756\n",
      "Epoch 191/200, Loss: 0.4238\n",
      "Epoch 192/200, Loss: 0.5927\n",
      "Epoch 193/200, Loss: 0.4484\n",
      "Epoch 194/200, Loss: 0.3914\n",
      "Epoch 195/200, Loss: 0.5056\n",
      "Epoch 196/200, Loss: 0.3574\n",
      "Epoch 197/200, Loss: 0.7052\n",
      "Epoch 198/200, Loss: 0.4266\n",
      "Epoch 199/200, Loss: 0.3353\n",
      "Epoch 200/200, Loss: 0.4248\n",
      "Training Accuracy: 90.93%\n",
      "Test Accuracy: 91.12%\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)\n",
    "    \n",
    "train_Y = one_hot_encode(train_Y)\n",
    "test_Y_one_hot = one_hot_encode(test_Y)\n",
    "    \n",
    "input_dim = 784\n",
    "intermediary_dim = 100\n",
    "output_dim = 10\n",
    "epochs = 200\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "    \n",
    "W1, b1, W2, b2 = train_model(train_X, train_Y, input_dim, intermediary_dim,output_dim, epochs, learning_rate, batch_size)\n",
    "    \n",
    "train_predictions = predict(train_X, W1, b1, W2, b2)\n",
    "test_predictions = predict(test_X, W1, b1, W2, b2)\n",
    "    \n",
    "print(f'Training Accuracy: {accuracy_score(np.argmax(train_Y, axis=1), train_predictions) * 100:.2f}%')\n",
    "print(f'Test Accuracy: {accuracy_score(test_Y, test_predictions) * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
